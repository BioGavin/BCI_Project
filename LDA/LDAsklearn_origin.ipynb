{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn-LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码示例：https://mp.weixin.qq.com/s/hMcJtB3Lss1NBalXRTGZlQ （玉树芝兰） <br>\n",
    "可视化：https://blog.csdn.net/qq_39496504/article/details/107125284  <br>\n",
    "sklearn lda参数解读:https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "<br>中文版参数解读：https://blog.csdn.net/TiffanyRabbit/article/details/76445909\n",
    "<br>LDA原理-视频版：https://www.bilibili.com/video/BV1t54y127U8\n",
    "<br>LDA原理-文字版：https://www.jianshu.com/p/5c510694c07e\n",
    "<br>score的计算方法：https://github.com/scikit-learn/scikit-learn/blob/844b4be24d20fc42cc13b957374c718956a0db39/sklearn/decomposition/_lda.py#L729\n",
    "<br>主题困惑度1：https://blog.csdn.net/weixin_43343486/article/details/109255165\n",
    "<br>主题困惑度2：https://blog.csdn.net/weixin_39676021/article/details/112187210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "import jieba.posseg as psg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                                               content type\n0    马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军\\n　　记者傅亚雨沈阳报道 来到沈阳，国奥队...   体育\n1    商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典\\n　　多曼来了，瑞典来了，商瑞华首战求3分...   体育\n2    冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛\\n　　新浪体育讯　12月27日晚，“冠军高尔...   体育\n3    辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀\\n　　新浪体育讯　2月24日，辽足爆发了集...   体育\n4    揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹\\n　　体坛周报特约记者张锐北京报道  谢亚龙已...   体育\n..                                                 ...  ...\n795  12月9日美股专家坐堂实录\\n　　12月9日，美股专家warren王做客新浪财经，回答新浪网...   股票\n796  12月10日美股专家坐堂实录\\n　　12月10日，美股专家warren王做客新浪财经，回答新...   股票\n797  12月15日美股专家坐堂实录\\n　　12月15日，美股专家Aries高做客新浪财经，回答新浪...   股票\n798  12月17日美股专家坐堂实录\\n　　12月17日，美股专家Aries高做客新浪财经，回答新浪...   股票\n799  12月18日美股专家坐堂实录\\n　　12月18日，美股专家Aries高做客新浪财经，回答新浪...   股票\n\n[800 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>马晓旭意外受伤让国奥警惕 无奈大雨格外青睐殷家军\\n　　记者傅亚雨沈阳报道 来到沈阳，国奥队...</td>\n      <td>体育</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>商瑞华首战复仇心切 中国玫瑰要用美国方式攻克瑞典\\n　　多曼来了，瑞典来了，商瑞华首战求3分...</td>\n      <td>体育</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>冠军球队迎新欢乐派对 黄旭获大奖张军赢下PK赛\\n　　新浪体育讯　12月27日晚，“冠军高尔...</td>\n      <td>体育</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>辽足签约危机引注册难关 高层威逼利诱合同笑里藏刀\\n　　新浪体育讯　2月24日，辽足爆发了集...</td>\n      <td>体育</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>揭秘谢亚龙被带走：总局电话骗局 复制南杨轨迹\\n　　体坛周报特约记者张锐北京报道  谢亚龙已...</td>\n      <td>体育</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>12月9日美股专家坐堂实录\\n　　12月9日，美股专家warren王做客新浪财经，回答新浪网...</td>\n      <td>股票</td>\n    </tr>\n    <tr>\n      <th>796</th>\n      <td>12月10日美股专家坐堂实录\\n　　12月10日，美股专家warren王做客新浪财经，回答新...</td>\n      <td>股票</td>\n    </tr>\n    <tr>\n      <th>797</th>\n      <td>12月15日美股专家坐堂实录\\n　　12月15日，美股专家Aries高做客新浪财经，回答新浪...</td>\n      <td>股票</td>\n    </tr>\n    <tr>\n      <th>798</th>\n      <td>12月17日美股专家坐堂实录\\n　　12月17日，美股专家Aries高做客新浪财经，回答新浪...</td>\n      <td>股票</td>\n    </tr>\n    <tr>\n      <th>799</th>\n      <td>12月18日美股专家坐堂实录\\n　　12月18日，美股专家Aries高做客新浪财经，回答新浪...</td>\n      <td>股票</td>\n    </tr>\n  </tbody>\n</table>\n<p>800 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_path = 'D:/python/lda/result'\n",
    "# file_path = 'D:/python/lda/data'\n",
    "# os.chdir(file_path)\n",
    "data=pd.read_excel(\"data.xlsx\")#content type\n",
    "# os.chdir(output_path)\n",
    "dic_file = \"dict.txt\"\n",
    "stop_file = \"stopwords.txt\"\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_word_cut(mytext):\n",
    "    jieba.load_userdict(dic_file)\n",
    "    jieba.initialize()\n",
    "    try:\n",
    "        stopword_list = open(stop_file,encoding ='utf-8')\n",
    "    except:\n",
    "        stopword_list = []\n",
    "        print(\"error in stop_file\")\n",
    "    stop_list = []\n",
    "    flag_list = ['n','nz','vn']\n",
    "    for line in stopword_list:\n",
    "        line = re.sub(u'\\n|\\\\r', '', line)\n",
    "        stop_list.append(line)\n",
    "    \n",
    "    word_list = []\n",
    "    #jieba分词\n",
    "    seg_list = psg.cut(mytext)\n",
    "    for seg_word in seg_list:\n",
    "        word = re.sub(u'[^\\u4e00-\\u9fa5]','',seg_word.word)\n",
    "        #word = seg_word.word  #如果想要分析英语文本，注释这行代码，启动下行代码\n",
    "        find = 0\n",
    "        for stop_word in stop_list:\n",
    "            if stop_word == word or len(word)<2:     #this word is stopword\n",
    "                    find = 1\n",
    "                    break\n",
    "        if find == 0 and seg_word.flag in flag_list:\n",
    "            word_list.append(word)      \n",
    "    return (\" \").join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/3c/v18vsqt50jzgnmgzv858z_rm0000gn/T/jieba.cache\n",
      "Loading model cost 0.487 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "data[\"content_cutted\"] = data.content.apply(chinese_word_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.LDA分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    tword = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        topic_w = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        tword.append(topic_w)\n",
    "        print(topic_w)\n",
    "    return tword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000 #提取1000个特征词语\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                max_df = 0.5,\n",
    "                                min_df = 10)\n",
    "tf = tf_vectorizer.fit_transform(data.content_cutted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LatentDirichletAllocation(learning_offset=50, max_iter=50, n_components=8,\n                          random_state=0)",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_offset=50, max_iter=50, n_components=8,\n                          random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_offset=50, max_iter=50, n_components=8,\n                          random_state=0)</pre></div></div></div></div></div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics = 8\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50,\n",
    "#                                 doc_topic_prior=0.1,\n",
    "#                                 topic_word_prior=0.01,\n",
    "                               random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1输出每个主题对应词语 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m n_top_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m25\u001B[39m\n\u001B[0;32m----> 2\u001B[0m tf_feature_names \u001B[38;5;241m=\u001B[39m \u001B[43mtf_vectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_feature_names\u001B[49m()\n\u001B[1;32m      3\u001B[0m topic_word \u001B[38;5;241m=\u001B[39m print_top_words(lda, tf_feature_names, n_top_words)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "n_top_words = 25\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "topic_word = print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2输出每篇文章对应主题 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = []\n",
    "for t in topics:\n",
    "    topic.append(\"Topic #\"+str(list(t).index(np.max(t))))\n",
    "data['概率最大的主题序号']=topic\n",
    "data['每个主题对应概率']=list(topics)\n",
    "data.to_excel(\"data_topic.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3可视化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pic = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.display(pic)\n",
    "pyLDAvis.save_html(pic, 'lda_pass'+str(n_topics)+'.html')\n",
    "pyLDAvis.display(pic)\n",
    "#去工作路径下找保存好的html文件\n",
    "#和视频里讲的不一样，目前这个代码不需要手动中断运行，可以快速出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4困惑度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plexs = []\n",
    "scores = []\n",
    "n_max_topics = 16\n",
    "for i in range(1,n_max_topics):\n",
    "    print(i)\n",
    "    lda = LatentDirichletAllocation(n_components=i, max_iter=50,\n",
    "                                    learning_method='batch',\n",
    "                                    learning_offset=50,random_state=0)\n",
    "    lda.fit(tf)\n",
    "    plexs.append(lda.perplexity(tf))\n",
    "    scores.append(lda.score(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t=15#区间最右侧的值。注意：不能大于n_max_topics\n",
    "x=list(range(1,n_t+1))\n",
    "plt.plot(x,plexs[0:n_t])\n",
    "plt.xlabel(\"number of topics\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "303.324px",
    "left": "114px",
    "top": "110.322px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
